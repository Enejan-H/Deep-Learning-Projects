{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4737381,"sourceType":"datasetVersion","datasetId":2740486},{"sourceId":7870110,"sourceType":"datasetVersion","datasetId":4617783},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n<center><img src=\"https://cdn.mos.cms.futurecdn.net/RNrVwVfRiyoKkrr8djHvf9-650-80.jpg.webp\"></center>\n<center><font color=\"BBBBBB\" size=2>(Image credit: Adobe Firefly - AI generated for Future, from Tom's guide)</font></center>\n\n## Objective\n\nUse Llama3 Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\nWhen using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \nThe data that we will use is the text of EU AI Act, approved on March 13, 2024.\n\n## Definitions\n\n* LLM - Large Language Model  \n* Llama3- LLM from Meta \n* Langchain - a framework designed to simplify the creation of applications using LLMs\n* Vector database - a database that organizes data through high-dimmensional vectors  \n* ChromaDB - vector database  \n* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n\n## Model details\n\n* **Model**: Llama 3  \n* **Variation**: 8b-chat-hf  (8b: 8B dimm.; hf: HuggingFace)\n* **Version**: V1  \n* **Framework**: Transformers  \n\nLlama3 model is pretrained and fine-tuned with 15T+ (more than 15 Trillion) tokens and 8 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over Llama2 model.\n\n\n## What is a Retrieval Augmented Generation (RAG) system?\n\nLarge Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n \nThe retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n\nFor the generator part, the obvious option is a LLM. In this Notebook we will use a quantized Llama3 model, from the Kaggle Models collection.  \n\nThe orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n\n## The data \n\nThe data that will be indexed in the vector database to make it searchable by the RAG system is the complete text of the European Union Artificial Intelligence Act. This is a European Union regulation on Artificial Intelligence (AI) in the European Union. Proposed by the European Commission on 21 April 2021, it was adopted on 13 March 2024. \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T13:32:56.898215Z","iopub.execute_input":"2024-08-10T13:32:56.898532Z","iopub.status.idle":"2024-08-10T13:35:45.79871Z","shell.execute_reply.started":"2024-08-10T13:32:56.898504Z","shell.execute_reply":"2024-08-10T13:35:45.797269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n#import chromadb\n#from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T13:35:45.800737Z","iopub.execute_input":"2024-08-10T13:35:45.801065Z","iopub.status.idle":"2024-08-10T13:35:52.841255Z","shell.execute_reply.started":"2024-08-10T13:35:45.801034Z","shell.execute_reply":"2024-08-10T13:35:52.840494Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize model, tokenizer, query pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\nprint(device)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:35:52.842383Z","iopub.execute_input":"2024-08-10T13:35:52.842824Z","iopub.status.idle":"2024-08-10T13:35:52.911863Z","shell.execute_reply.started":"2024-08-10T13:35:52.842797Z","shell.execute_reply":"2024-08-10T13:35:52.910884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_start = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n   model_id,\n    trust_remote_code=True,\n    max_new_tokens=1024\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_end = time()\nprint(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:35:52.914386Z","iopub.execute_input":"2024-08-10T13:35:52.915143Z","iopub.status.idle":"2024-08-10T13:37:42.451004Z","shell.execute_reply.started":"2024-08-10T13:35:52.915114Z","shell.execute_reply":"2024-08-10T13:37:42.450023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Netx, we define the query pipeline.  \nIn order to work correctly when we will define the HuggingFace pipeline, we will need to define here the max_length (to avoid falling back on the very short default length of `20`.","metadata":{}},{"cell_type":"code","source":"time_start = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        max_length=1024,\n        device_map=\"auto\",)\ntime_end = time()\nprint(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:37:42.452191Z","iopub.execute_input":"2024-08-10T13:37:42.452496Z","iopub.status.idle":"2024-08-10T13:37:44.31962Z","shell.execute_reply.started":"2024-08-10T13:37:42.452442Z","shell.execute_reply":"2024-08-10T13:37:44.318659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, message):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        message: the prompt\n    Returns\n        None\n    \"\"\"    \n    time_start = time()\n    sequences = pipeline(\n        message,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_end = time()\n    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n    \n    question = sequences[0]['generated_text'][:len(message)]\n    answer = sequences[0]['generated_text'][len(message):]\n    \n    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\"\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:37:44.321045Z","iopub.execute_input":"2024-08-10T13:37:44.321413Z","iopub.status.idle":"2024-08-10T13:37:44.328068Z","shell.execute_reply.started":"2024-08-10T13:37:44.321378Z","shell.execute_reply":"2024-08-10T13:37:44.32707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the query pipeline\n\nWe test the pipeline with few queries about European Union Artificial Intelligence Act (EU AI Act).","metadata":{}},{"cell_type":"markdown","source":"We also define here an utility function. This function will be used to display the output from the answer of the LLM.  \nWe include the calculation time, the question and the answer, formated so that will be easy to recognise them.","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, Markdown\ndef colorize_text(text):\n    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-10T13:37:44.329216Z","iopub.execute_input":"2024-08-10T13:37:44.329561Z","iopub.status.idle":"2024-08-10T13:37:44.350668Z","shell.execute_reply.started":"2024-08-10T13:37:44.32953Z","shell.execute_reply":"2024-08-10T13:37:44.349747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's test now the pipeline with few queries.","metadata":{}},{"cell_type":"code","source":"response = test_model(tokenizer,\n                    query_pipeline,\n                   \"Please explain what is EU AI Act.\")\ndisplay(Markdown(colorize_text(response)))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:37:44.351833Z","iopub.execute_input":"2024-08-10T13:37:44.352135Z","iopub.status.idle":"2024-08-10T13:38:04.753936Z","shell.execute_reply.started":"2024-08-10T13:37:44.352105Z","shell.execute_reply":"2024-08-10T13:38:04.75302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = test_model(tokenizer,\n                    query_pipeline,\n                   \"In the context of EU AI Act, how is performed the testing of high-risk AI systems in real world conditions?\")\ndisplay(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T13:38:04.755176Z","iopub.execute_input":"2024-08-10T13:38:04.755443Z","iopub.status.idle":"2024-08-10T13:38:20.256786Z","shell.execute_reply.started":"2024-08-10T13:38:04.755419Z","shell.execute_reply":"2024-08-10T13:38:20.255827Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The answer is not really useful. Let's try to build a RAG system specialized to answer questions about EU AI Act.","metadata":{}},{"cell_type":"markdown","source":"# Retrieval Augmented Generation\n\nIn order to build the RAG system, we will perform the following steps:\n* Test the model using a HuggingFacePipeline;  \n* Ingest the document using PyPdfLoader;\n* Chunk the documents (with chunk size 1000), making sure we have also a partial overlap (of 100 characters);  \n* Create embeddings and ingest the transformed text (text from pdf, chunked with overlap, embedded, and indexed) in the vector database;  \n* Create the RequestQA pipeline (that includes the retrieval step and the generation step).","metadata":{}},{"cell_type":"markdown","source":"## Check the model with a HuggingFace pipeline\n\n\nWe check the model with a HF pipeline, using a query about the meaning of EU AI Act. We will need to use the HuggingFacePipeline in order to integrate easier with the Langchain tasks.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n\n# checking again that everything is working fine\ntime_start = time()\nquestion = \"Please explain what EU AI Act is.\"\nresponse = llm(prompt=question)\ntime_end = time()\ntotal_time = f\"{round(time_end-time_start, 3)} sec.\"\nfull_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\ndisplay(Markdown(colorize_text(full_response)))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:38:20.260697Z","iopub.execute_input":"2024-08-10T13:38:20.26138Z","iopub.status.idle":"2024-08-10T13:39:46.423943Z","shell.execute_reply.started":"2024-08-10T13:38:20.261351Z","shell.execute_reply":"2024-08-10T13:39:46.42302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ingestion of data using PyPDFLoader\n\nWe will ingest the EU AI Act data using the PyPDFLoader from Langchain. There are actually multiple PDF ingestion utilities, we selected this one since it is easy to use.","metadata":{}},{"cell_type":"code","source":"loader = PyPDFLoader(\"/kaggle/input/eu-ai-act-complete-text/aiact_final_draft.pdf\")\ndocuments = loader.load()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:39:46.42516Z","iopub.execute_input":"2024-08-10T13:39:46.425436Z","iopub.status.idle":"2024-08-10T13:40:04.661649Z","shell.execute_reply.started":"2024-08-10T13:39:46.425412Z","shell.execute_reply":"2024-08-10T13:40:04.66064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.  \n\nNote: You can experiment with several values of chunk_size and chunk_overlap. Here we will set the following values:\n* chunk_size: 1000 (this gives the size of a chunk, in characters). \n* chunk_overlap: 100 (this gives the number of characters with which two succesive chunks overlap).  \n\nChunk overlapping is required in order to be able to keep the context, even if we have a concept that we want to include that is spread over multiple document chunks.\n","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:04.662871Z","iopub.execute_input":"2024-08-10T13:40:04.663178Z","iopub.status.idle":"2024-08-10T13:40:04.742629Z","shell.execute_reply.started":"2024-08-10T13:40:04.663152Z","shell.execute_reply":"2024-08-10T13:40:04.741838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.  \nOcasionally, HuggingFace sentence-transformers might not be available. We implement therefore a mechanism to work with local stored sentence transformers.","metadata":{}},{"cell_type":"code","source":"\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\n# try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2\ntry:\n    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\nexcept Exception as ex:\n    print(\"Exception: \", ex)\n    # alternatively, we will access the embeddings models locally\n    local_model_path = \"/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2\"\n    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n    embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:04.743775Z","iopub.execute_input":"2024-08-10T13:40:04.74407Z","iopub.status.idle":"2024-08-10T13:40:13.881684Z","shell.execute_reply.started":"2024-08-10T13:40:04.744045Z","shell.execute_reply":"2024-08-10T13:40:13.880657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.  \nWe make sure to use the persistence option for the vector database.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:13.883004Z","iopub.execute_input":"2024-08-10T13:40:13.883311Z","iopub.status.idle":"2024-08-10T13:40:23.217795Z","shell.execute_reply.started":"2024-08-10T13:40:13.883282Z","shell.execute_reply":"2024-08-10T13:40:23.216957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize chain   \n\nWe are using `RetrievalQA` task chain utility from Langchain.  \nThis will first query the vector database (using similarity search) with the prompt we are using.   \nThen, the query and the context retrieved (the documents that match with the query) are used to compose a prompt that instructs the LLM to answer to the query (**Generation**) using the information from the context retrieved (**Retrieval**). Therefore the name of the system, `Retrieval Augmented Generation`. \n","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:23.219296Z","iopub.execute_input":"2024-08-10T13:40:23.219606Z","iopub.status.idle":"2024-08-10T13:40:23.224672Z","shell.execute_reply.started":"2024-08-10T13:40:23.219578Z","shell.execute_reply":"2024-08-10T13:40:23.223632Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    \"\"\"\n    Test the Retrieval Augmented Generation (RAG) system.\n    \n    Args:\n        qa (RetrievalQA.from_chain_type): Langchain function to perform RAG\n        query (str): query for the RAG system\n    Returns:\n        None\n    \"\"\"\n\n    time_start = time()\n    response = qa.run(query)\n    time_end = time()\n    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n\n    full_response =  f\"Question: {query}\\nAnswer: {response}\\nTotal time: {total_time}\"\n    display(Markdown(colorize_text(full_response)))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:23.225916Z","iopub.execute_input":"2024-08-10T13:40:23.226202Z","iopub.status.idle":"2024-08-10T13:40:23.238299Z","shell.execute_reply.started":"2024-08-10T13:40:23.226177Z","shell.execute_reply":"2024-08-10T13:40:23.237351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query = \"How is performed the testing of high-risk AI systems in real world conditions?\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-08-10T13:40:23.240166Z","iopub.execute_input":"2024-08-10T13:40:23.241104Z","iopub.status.idle":"2024-08-10T13:40:52.726296Z","shell.execute_reply.started":"2024-08-10T13:40:23.24107Z","shell.execute_reply":"2024-08-10T13:40:52.725327Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"What are the operational obligations of notified bodies?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T13:40:52.72751Z","iopub.execute_input":"2024-08-10T13:40:52.727819Z","iopub.status.idle":"2024-08-10T13:41:18.629802Z","shell.execute_reply.started":"2024-08-10T13:40:52.72779Z","shell.execute_reply":"2024-08-10T13:41:18.628789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"What are the unacceptable risks?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T13:41:18.631239Z","iopub.execute_input":"2024-08-10T13:41:18.631622Z","iopub.status.idle":"2024-08-10T13:42:21.568889Z","shell.execute_reply.started":"2024-08-10T13:41:18.631587Z","shell.execute_reply":"2024-08-10T13:42:21.567939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"In what cases a company that develops AI solutions should obtain permission to deploy it?\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T13:42:21.570108Z","iopub.execute_input":"2024-08-10T13:42:21.570414Z","iopub.status.idle":"2024-08-10T13:42:58.546285Z","shell.execute_reply.started":"2024-08-10T13:42:21.570387Z","shell.execute_reply":"2024-08-10T13:42:58.545345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.  \n\nIn order to do this, we will perform the following steps:\n* We run a similarity search in the vector database;\n* We loop through the documents returned; \n* Print, for each document, the documents source, from the metadata, and the page content.\n","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T13:44:28.371263Z","iopub.execute_input":"2024-08-10T13:44:28.372205Z","iopub.status.idle":"2024-08-10T13:44:28.417314Z","shell.execute_reply.started":"2024-08-10T13:44:28.372168Z","shell.execute_reply":"2024-08-10T13:44:28.416369Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusions\n\n\nWe used Langchain, ChromaDB and Llama3 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the EU AI Act from 2023.  \nThe answers to questions from EU AI Act are correct, when using a RAG model.  \n\nTo improve the solution, we will have to refine the RAG implementation, first by optimizing the embeddings, then by using more complex RAG schemes.\n\n\n\n","metadata":{}}]}